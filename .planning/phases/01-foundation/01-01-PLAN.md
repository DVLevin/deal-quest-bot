---
phase: 01-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - bot/tracing/__init__.py
  - bot/tracing/context.py
  - bot/tracing/collector.py
  - bot/tracing/models.py
  - bot/storage/repositories.py
  - bot/storage/models.py
  - insforge/migrations/001_pipeline_traces.sql
autonomous: true

must_haves:
  truths:
    - "TraceContext async context manager can be entered and exited, recording start/end time and duration"
    - "ContextVar trace_id propagates across async boundaries (asyncio.gather, create_task) — verified by inline behavioral test"
    - "Spans can be recorded via traced_span decorator with timing, parent linkage, and I/O capture"
    - "TraceCollector buffers traces/spans and flushes to InsForge via TraceRepo — verified by background flush behavioral test"
    - "traced_span decorator is ready to be applied to agent .run() methods and LLM .complete() calls (Plan 01-03)"
    - "pipeline_traces and pipeline_spans tables exist in InsForge with correct schema and indexes"
  artifacts:
    - path: "bot/tracing/__init__.py"
      provides: "Public API: TraceContext, traced_span, init_collector, get_collector"
    - path: "bot/tracing/context.py"
      provides: "TraceContext context manager, traced_span decorator, ContextVar propagation"
      min_lines: 80
    - path: "bot/tracing/collector.py"
      provides: "TraceCollector with batched background flush"
      min_lines: 60
    - path: "bot/tracing/models.py"
      provides: "TraceModel and SpanModel Pydantic models"
      min_lines: 20
    - path: "bot/storage/repositories.py"
      provides: "TraceRepo class for InsForge persistence"
      contains: "class TraceRepo"
    - path: "insforge/migrations/001_pipeline_traces.sql"
      provides: "DDL for pipeline_traces, pipeline_spans tables with indexes and RLS"
      min_lines: 30
  key_links:
    - from: "bot/tracing/context.py"
      to: "bot/tracing/collector.py"
      via: "get_collector() call in __aexit__"
      pattern: "get_collector"
    - from: "bot/tracing/collector.py"
      to: "bot/storage/repositories.py"
      via: "TraceRepo.create_trace and create_span"
      pattern: "trace_repo"
    - from: "bot/tracing/context.py"
      to: "contextvars"
      via: "ContextVar for trace_id propagation"
      pattern: "ContextVar"
---

<objective>
Build the core tracing module and storage layer for pipeline observability.

Purpose: Create the foundational TraceContext context manager (mirroring ProgressUpdater pattern), ContextVar-based trace propagation, span recording decorator, batched collector, and InsForge storage — everything needed before instrumenting handlers and agent internals.

Output: `bot/tracing/` module with TraceContext, traced_span, TraceCollector; TraceRepo in storage; SQL migration for InsForge tables.
</objective>

<execution_context>
@/Users/dmytrolevin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dmytrolevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-RESEARCH.md

@bot/services/progress.py — ProgressUpdater pattern to mirror (async context manager wrapping pipeline calls)
@bot/storage/insforge_client.py — InsForgeClient API (create, query, update, rpc methods)
@bot/storage/repositories.py — Repository pattern to follow (UserRepo, AttemptRepo, etc.)
@bot/storage/models.py — Pydantic model pattern to follow
@bot/pipeline/runner.py — PipelineRunner internals (DO NOT modify, understand for span naming)
@bot/pipeline/context.py — PipelineContext (carries telegram_id, user_id)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create tracing models and TraceContext with traced_span decorator</name>
  <files>
    bot/tracing/__init__.py
    bot/tracing/context.py
    bot/tracing/models.py
  </files>
  <action>
Create `bot/tracing/` package with three files:

**bot/tracing/models.py** — Pydantic data models following bot/storage/models.py pattern:
- `TraceModel`: trace_id (str/UUID), pipeline_name (str), telegram_id (int), user_id (int), start_time (str ISO), end_time (str ISO), duration_ms (float), success (bool), error (str|None), created_at (str|None)
- `SpanModel`: span_id (str/UUID), trace_id (str), parent_span_id (str|None), span_name (str), start_time (str ISO), end_time (str ISO), duration_ms (float), success (bool), error (str|None), input_data (dict|None), output_data (dict|None), created_at (str|None)

**bot/tracing/context.py** — Core tracing primitives:
- Module-level ContextVars: `_trace_id: ContextVar[str | None]` (default None), `_span_stack: ContextVar[list[str] | None]` (default None)
- `TraceContext` async context manager:
  - `__init__(self, pipeline_name: str, telegram_id: int, user_id: int)` — generates trace_id via uuid.uuid4(), stores metadata
  - `__aenter__` — sets _trace_id and _span_stack ContextVars, records wall-clock start_time via `datetime.now(timezone.utc)`, records perf start via `time.perf_counter()`
  - `__aexit__` — records end time (both wall-clock and perf_counter), computes duration_ms from perf_counter diff, captures exception info if any, calls `get_collector().record_trace(self)` if collector exists, clears ContextVars. Does NOT suppress exceptions (returns None).
  - Important: Store both `datetime.now(timezone.utc)` timestamps (for DB storage) AND `time.perf_counter()` values (for duration calculation). Don't mix them.
- `traced_span(span_name: str = None)` decorator factory:
  - Returns async decorator that wraps async functions
  - On entry: reads _trace_id from ContextVar, if None returns early (not in traced context). Creates span_id via uuid.uuid4(), records wall-clock + perf_counter start. Reads _span_stack for parent_span_id (last item or None). Pushes span_id onto stack.
  - On success: records end time, calls `get_collector().record_span(...)` with all data including input_data from kwargs and output_data from result (serialize safely — wrap in try/except, fall back to str(result) if not serializable)
  - On exception: records end time, records span with success=False and error string, re-raises
  - Finally: pops span_id from stack
  - For input_data capture: Use a helper `_safe_serialize(obj)` that tries `obj` directly if it's a dict/list/str/int/float/bool/None, tries `.model_dump()` for Pydantic models, falls back to `{"repr": repr(obj)[:2000]}`. Limit total serialized size to 50KB to prevent storage explosion.
  - For output_data: Same `_safe_serialize` treatment
- Helper: `get_current_trace_id() -> str | None` — reads _trace_id ContextVar

**bot/tracing/__init__.py** — Initial public API exports (collector exports added in Task 2):
```python
from bot.tracing.context import TraceContext, traced_span, get_current_trace_id
```
  </action>
  <verify>
    - `python -c "from bot.tracing import TraceContext, traced_span, get_current_trace_id"` succeeds
    - `python -c "from bot.tracing.models import TraceModel, SpanModel; t = TraceModel(trace_id='x', pipeline_name='test', telegram_id=1, user_id=1, start_time='2026-01-01T00:00:00Z', end_time='2026-01-01T00:00:01Z', duration_ms=1000.0, success=True); print(t.trace_id)"` prints "x"
    - Behavioral: ContextVar propagation test:
```bash
python -c "
import asyncio
from bot.tracing.context import TraceContext, _trace_id, _span_stack

async def test():
    # Test 1: ContextVar set inside, cleared outside
    async with TraceContext('test', 123, 1) as t:
        assert _trace_id.get() == t.trace_id, 'trace_id not set inside context'
        assert _span_stack.get() is not None, 'span_stack not set inside context'
    assert _trace_id.get() is None, 'trace_id not cleared after exit'
    print('Test 1 PASSED: ContextVar set/clear')

    # Test 2: Propagation across asyncio.gather
    async with TraceContext('test2', 456, 2) as t:
        tid = t.trace_id
        async def child(n):
            assert _trace_id.get() == tid, f'child {n} lost trace_id'
            return n
        results = await asyncio.gather(child(1), child(2), child(3))
        assert results == [1, 2, 3]
    print('Test 2 PASSED: ContextVar propagates across asyncio.gather')

asyncio.run(test())
"
```
  </verify>
  <done>
    - TraceContext can be instantiated and used as async context manager, setting/clearing ContextVars
    - ContextVar trace_id propagates correctly across asyncio.gather boundaries
    - traced_span decorator wraps async functions and records spans when trace_id is set
    - TraceModel and SpanModel validate trace/span data
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TraceCollector with batched background flush</name>
  <files>
    bot/tracing/collector.py
    bot/tracing/__init__.py
  </files>
  <action>
**bot/tracing/collector.py** — Batched background flush:
- `TraceCollector` class:
  - `__init__(self, trace_repo, batch_size=50, flush_interval=10.0)` — stores repo ref, creates deque buffers for traces and spans, asyncio.Event for stop, task ref
  - `async start()` — creates background flush loop task via asyncio.create_task
  - `async stop()` — sets stop event, awaits task, flushes remaining
  - `async record_trace(trace_context)` — converts TraceContext to TraceModel, appends to buffer, flushes if buffer >= batch_size
  - `async record_span(**span_data)` — converts to SpanModel, appends to buffer, flushes if buffer >= batch_size
  - `async _flush_loop()` — waits on stop event with timeout=flush_interval, flushes on timeout
  - `async _flush_now()` — drains both buffers, calls trace_repo.create_trace / create_span for each item, logs errors but doesn't crash
- Module-level singleton: `_collector: TraceCollector | None = None`
- `init_collector(trace_repo) -> TraceCollector` — creates and returns singleton
- `get_collector() -> TraceCollector | None` — returns singleton

**bot/tracing/__init__.py** — Update to include collector exports:
```python
from bot.tracing.context import TraceContext, traced_span, get_current_trace_id
from bot.tracing.collector import init_collector, get_collector
```
  </action>
  <verify>
    - `python -c "from bot.tracing import TraceContext, traced_span, init_collector, get_collector"` succeeds
    - Behavioral: Background flush test:
```bash
python -c "
import asyncio
from unittest.mock import AsyncMock, MagicMock
from bot.tracing.collector import TraceCollector

async def test():
    # Create mock repo
    mock_repo = MagicMock()
    mock_repo.create_trace = AsyncMock()
    mock_repo.create_span = AsyncMock()

    # Create collector with short flush interval
    collector = TraceCollector(mock_repo, batch_size=50, flush_interval=0.1)
    await collector.start()

    # Record a mock trace
    mock_trace = MagicMock()
    mock_trace.trace_id = 'test-123'
    mock_trace.pipeline_name = 'test'
    mock_trace.telegram_id = 123
    mock_trace.user_id = 1
    mock_trace.start_time_wall = '2026-01-01T00:00:00+00:00'
    mock_trace.end_time_wall = '2026-01-01T00:00:01+00:00'
    mock_trace.duration_ms = 1000.0
    mock_trace.success = True
    mock_trace.error = None
    await collector.record_trace(mock_trace)

    # Stop collector (should flush remaining)
    await collector.stop()

    # Verify flush happened
    assert mock_repo.create_trace.called, 'create_trace was never called'
    print('Test PASSED: TraceCollector flushes on stop')

asyncio.run(test())
"
```
  </verify>
  <done>
    - TraceCollector buffers traces/spans and flushes via background task
    - stop() flushes remaining buffered items before returning
    - init_collector / get_collector singleton pattern works
    - All public API exports resolve from bot.tracing
  </done>
</task>

<task type="auto">
  <name>Task 3: Create TraceRepo and add Pydantic storage models</name>
  <files>
    bot/storage/repositories.py
    bot/storage/models.py
  </files>
  <action>
**bot/storage/models.py** — Add two new models at the end of the file (after GeneratedScenarioModel). Follow existing pattern exactly:

```python
class PipelineTraceModel(BaseModel):
    id: int | None = None
    trace_id: str
    pipeline_name: str
    telegram_id: int
    user_id: int | None = None
    start_time: str
    end_time: str
    duration_ms: float
    success: bool = True
    error: str | None = None
    created_at: str | None = None

class PipelineSpanModel(BaseModel):
    id: int | None = None
    span_id: str
    trace_id: str
    parent_span_id: str | None = None
    span_name: str
    start_time: str
    end_time: str
    duration_ms: float
    success: bool = True
    error: str | None = None
    input_data: dict[str, Any] | None = None
    output_data: dict[str, Any] | None = None
    created_at: str | None = None
```

**bot/storage/repositories.py** — Add `TraceRepo` class at the end (after GeneratedScenarioRepo). Follow existing repository pattern:

```python
class TraceRepo:
    """Repository for pipeline traces and spans."""

    def __init__(self, client: InsForgeClient) -> None:
        self.client = client
        self.traces_table = "pipeline_traces"
        self.spans_table = "pipeline_spans"

    async def create_trace(self, trace: PipelineTraceModel) -> PipelineTraceModel | None:
        data = trace.model_dump(exclude_none=True, exclude={"id", "created_at"})
        result = await self.client.create(self.traces_table, data)
        return PipelineTraceModel(**result) if result else trace

    async def create_span(self, span: PipelineSpanModel) -> PipelineSpanModel | None:
        data = span.model_dump(exclude_none=True, exclude={"id", "created_at"})
        result = await self.client.create(self.spans_table, data)
        return PipelineSpanModel(**result) if result else span

    async def get_traces(
        self,
        *,
        telegram_id: int | None = None,
        pipeline_name: str | None = None,
        limit: int = 20,
    ) -> list[PipelineTraceModel]:
        filters: dict[str, Any] = {}
        if telegram_id is not None:
            filters["telegram_id"] = telegram_id
        if pipeline_name is not None:
            filters["pipeline_name"] = pipeline_name
        rows = await self.client.query(
            self.traces_table,
            filters=filters if filters else None,
            order="created_at.desc",
            limit=limit,
        )
        if rows and isinstance(rows, list):
            return [PipelineTraceModel(**r) for r in rows]
        return []

    async def get_spans_for_trace(self, trace_id: str) -> list[PipelineSpanModel]:
        rows = await self.client.query(
            self.spans_table,
            filters={"trace_id": trace_id},
            order="start_time.asc",
        )
        if rows and isinstance(rows, list):
            return [PipelineSpanModel(**r) for r in rows]
        return []
```

Add `PipelineTraceModel` and `PipelineSpanModel` to the imports at the top of repositories.py.
  </action>
  <verify>
    - `python -c "from bot.storage.models import PipelineTraceModel, PipelineSpanModel; print('OK')"` succeeds
    - `python -c "from bot.storage.repositories import TraceRepo; print('OK')"` succeeds
  </verify>
  <done>
    - PipelineTraceModel and PipelineSpanModel exist in bot/storage/models.py
    - TraceRepo exists in bot/storage/repositories.py with create_trace, create_span, get_traces, get_spans_for_trace methods
    - TraceRepo follows the same InsForgeClient pattern as all other repositories
  </done>
</task>

<task type="auto">
  <name>Task 4: Create InsForge SQL migration for traces and spans tables</name>
  <files>insforge/migrations/001_pipeline_traces.sql</files>
  <action>
Create `insforge/migrations/` directory if it doesn't exist.

Create `insforge/migrations/001_pipeline_traces.sql` with DDL for both tables. Since this is InsForge (PostgREST over PostgreSQL), the SQL will be executed via the InsForge dashboard or RPC. Do NOT use partitioning initially — InsForge may not support it, and the research recommends starting simple and adding partitioning reactively. Use simple tables with proper indexes:

```sql
-- Pipeline Traces — one row per pipeline execution
CREATE TABLE IF NOT EXISTS pipeline_traces (
    id SERIAL PRIMARY KEY,
    trace_id UUID NOT NULL UNIQUE,
    pipeline_name VARCHAR(50) NOT NULL,
    telegram_id BIGINT NOT NULL,
    user_id INTEGER,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE NOT NULL,
    duration_ms NUMERIC(10, 2) NOT NULL,
    success BOOLEAN NOT NULL DEFAULT true,
    error TEXT,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Pipeline Spans — one row per step within a trace
CREATE TABLE IF NOT EXISTS pipeline_spans (
    id SERIAL PRIMARY KEY,
    span_id UUID NOT NULL UNIQUE,
    trace_id UUID NOT NULL,
    parent_span_id UUID,
    span_name VARCHAR(100) NOT NULL,
    start_time TIMESTAMP WITH TIME ZONE NOT NULL,
    end_time TIMESTAMP WITH TIME ZONE NOT NULL,
    duration_ms NUMERIC(10, 2) NOT NULL,
    success BOOLEAN NOT NULL DEFAULT true,
    error TEXT,
    input_data JSONB,
    output_data JSONB,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Indexes for common query patterns
CREATE INDEX IF NOT EXISTS idx_traces_telegram_id ON pipeline_traces (telegram_id, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_traces_pipeline ON pipeline_traces (pipeline_name, created_at DESC);
CREATE INDEX IF NOT EXISTS idx_traces_created_at ON pipeline_traces (created_at DESC);
CREATE INDEX IF NOT EXISTS idx_spans_trace_id ON pipeline_spans (trace_id);
CREATE INDEX IF NOT EXISTS idx_spans_created_at ON pipeline_spans (created_at DESC);

-- Enable RLS (InsForge requirement)
ALTER TABLE pipeline_traces ENABLE ROW LEVEL SECURITY;
ALTER TABLE pipeline_spans ENABLE ROW LEVEL SECURITY;

-- Service role can do everything (bot uses service role key)
CREATE POLICY traces_service_all ON pipeline_traces FOR ALL USING (true) WITH CHECK (true);
CREATE POLICY spans_service_all ON pipeline_spans FOR ALL USING (true) WITH CHECK (true);
```

Note: Skip FK constraint from pipeline_spans.trace_id to pipeline_traces.trace_id — PostgREST works better without cross-table FKs for insert ordering, and the collector may insert spans before the trace record.
  </action>
  <verify>
    - File `insforge/migrations/001_pipeline_traces.sql` exists and contains CREATE TABLE statements for both pipeline_traces and pipeline_spans
    - SQL is syntactically valid (no obvious errors)
  </verify>
  <done>
    - SQL migration file ready for manual execution via InsForge dashboard
    - Tables have proper indexes for telegram_id, pipeline_name, and date range queries
    - RLS enabled with permissive service-role policies
  </done>
</task>

</tasks>

<verification>
Run all import checks:
```bash
cd deal-quest-bot
python -c "
from bot.tracing import TraceContext, traced_span, init_collector, get_collector, get_current_trace_id
from bot.tracing.models import TraceModel, SpanModel
from bot.storage.models import PipelineTraceModel, PipelineSpanModel
from bot.storage.repositories import TraceRepo
print('All imports OK')
"
```

Verify TraceContext ContextVar propagation (behavioral test):
```bash
python -c "
import asyncio
from bot.tracing.context import TraceContext, _trace_id

async def test():
    # Test: ContextVar propagates across asyncio.gather
    async with TraceContext('test', 123, 1) as t:
        tid = t.trace_id
        async def child(n):
            assert _trace_id.get() == tid, f'child {n} lost trace_id'
            return n
        results = await asyncio.gather(child(1), child(2), child(3))
        assert results == [1, 2, 3]
    assert _trace_id.get() is None
    print('ContextVar propagation test PASSED')

asyncio.run(test())
"
```

Verify TraceCollector flush behavior (behavioral test):
```bash
python -c "
import asyncio
from unittest.mock import AsyncMock, MagicMock
from bot.tracing.collector import TraceCollector

async def test():
    mock_repo = MagicMock()
    mock_repo.create_trace = AsyncMock()
    mock_repo.create_span = AsyncMock()
    collector = TraceCollector(mock_repo, batch_size=50, flush_interval=0.1)
    await collector.start()
    mock_trace = MagicMock()
    mock_trace.trace_id = 'test-123'
    mock_trace.pipeline_name = 'test'
    mock_trace.telegram_id = 123
    mock_trace.user_id = 1
    mock_trace.start_time_wall = '2026-01-01T00:00:00+00:00'
    mock_trace.end_time_wall = '2026-01-01T00:00:01+00:00'
    mock_trace.duration_ms = 1000.0
    mock_trace.success = True
    mock_trace.error = None
    await collector.record_trace(mock_trace)
    await collector.stop()
    assert mock_repo.create_trace.called
    print('TraceCollector flush test PASSED')

asyncio.run(test())
"
```
</verification>

<success_criteria>
- bot/tracing/ module exists with context.py, collector.py, models.py, __init__.py
- TraceContext is an async context manager that sets/clears ContextVars and records timing
- ContextVar trace_id propagates across asyncio.gather boundaries (behavioral test passes)
- traced_span decorator captures function I/O when inside a TraceContext
- TraceCollector buffers traces/spans and flushes via background task (flush test passes)
- TraceRepo persists to InsForge via InsForgeClient (same pattern as other repos)
- SQL migration file ready for InsForge table creation
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-01-SUMMARY.md`
</output>
