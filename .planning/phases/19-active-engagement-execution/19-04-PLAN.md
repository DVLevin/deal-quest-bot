---
phase: 19-active-engagement-execution
plan: 04
type: execute
wave: 1
depends_on: ["19-01", "19-02", "19-03"]
files_modified:
  - insforge/migrations/006_draft_requests.sql
  - bot/storage/models.py
  - bot/storage/repositories.py
  - bot/agents/comment_generator.py
  - prompts/comment_generator_agent.md
  - bot/services/draft_poller.py
  - bot/main.py
autonomous: true

must_haves:
  truths:
    - "Bot picks up pending draft_requests rows, processes them with CommentGeneratorAgent, and writes structured JSON result back"
    - "CommentGeneratorAgent auto-detects platform from screenshot and returns structured options (short/medium/detailed)"
    - "Admin can select model for comment_generator agent via TMA admin panel (ModelConfigService integration)"
    - "Langfuse traces every draft generation via @observe decorator"
    - "Stale processing requests are recovered on bot startup"
  artifacts:
    - path: "insforge/migrations/006_draft_requests.sql"
      provides: "draft_requests table with status, result JSONB, indexes"
      contains: "CREATE TABLE draft_requests"
    - path: "bot/agents/comment_generator.py"
      provides: "CommentGeneratorAgent with vision model support"
      contains: "class CommentGeneratorAgent"
    - path: "prompts/comment_generator_agent.md"
      provides: "Multi-platform auto-detection prompt for draft generation"
      contains: "auto-detect"
    - path: "bot/services/draft_poller.py"
      provides: "Background poller for draft_requests table"
      contains: "start_draft_request_poller"
    - path: "bot/storage/repositories.py"
      provides: "DraftRequestRepo with claim_next_pending"
      contains: "class DraftRequestRepo"
  key_links:
    - from: "bot/services/draft_poller.py"
      to: "bot/agents/comment_generator.py"
      via: "agent_registry.get('comment_generator')"
      pattern: "comment_generator"
    - from: "bot/services/draft_poller.py"
      to: "bot/storage/repositories.py"
      via: "DraftRequestRepo claim_next_pending"
      pattern: "claim_next_pending"
    - from: "bot/main.py"
      to: "bot/services/draft_poller.py"
      via: "create_background_task(start_draft_request_poller(...))"
      pattern: "draft_request_poller"
    - from: "bot/main.py"
      to: "bot/agents/comment_generator.py"
      via: "agent_registry.register(CommentGeneratorAgent())"
      pattern: "CommentGeneratorAgent"
---

<objective>
Build the complete bot-side draft generation pipeline: database table, repository, CommentGeneratorAgent with multi-platform prompt, background poller service, and main.py wiring.

Purpose: This migrates draft generation from the InsForge Edge Function (hardcoded model, no tracing, LinkedIn-only) to the bot's agent pipeline system (ModelConfigService, Langfuse tracing, multi-platform auto-detection). The DB message bus pattern enables async TMA-to-bot communication without adding an HTTP server to the bot.

Output: A running background poller that picks up `draft_requests` rows, processes them through CommentGeneratorAgent with vision model support, and writes structured JSON results (platform, content_type, options array) back to the database.
</objective>

<execution_context>
@/Users/dmytrolevin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dmytrolevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-active-engagement-execution/19-RESEARCH.md

@bot/agents/base.py
@bot/agents/extraction.py
@bot/agents/registry.py
@bot/pipeline/context.py
@bot/services/llm_router.py
@bot/services/image_utils.py
@bot/services/model_config.py
@bot/storage/insforge_client.py
@bot/storage/repositories.py
@bot/storage/models.py
@bot/task_utils.py
@bot/main.py
@functions/generate-draft/deploy.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create draft_requests migration, Pydantic model, and DraftRequestRepo</name>
  <files>
    insforge/migrations/006_draft_requests.sql
    bot/storage/models.py
    bot/storage/repositories.py
  </files>
  <action>
**1. Create migration file** `insforge/migrations/006_draft_requests.sql`:

```sql
-- Draft generation request queue (TMA -> Bot async message bus)
CREATE TABLE draft_requests (
  id BIGSERIAL PRIMARY KEY,
  lead_id BIGINT NOT NULL,
  step_id INT NOT NULL,
  telegram_id BIGINT NOT NULL,
  proof_url TEXT NOT NULL,
  lead_context JSONB DEFAULT '{}',
  status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'processing', 'completed', 'failed')),
  result JSONB DEFAULT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Index for efficient polling (bot checks for pending requests)
CREATE INDEX idx_draft_requests_pending ON draft_requests(status) WHERE status = 'pending';

-- Index for TMA polling (check completion by lead_id + step_id)
CREATE INDEX idx_draft_requests_lead_step ON draft_requests(lead_id, step_id, created_at DESC);
```

Note: No foreign key to `lead_registry` — PostgREST doesn't require it and the table may be on a different schema. `lead_id` is used for TMA correlation only.

**2. Add DraftRequestModel to `bot/storage/models.py`:**

At the end of the file, add:

```python
class DraftRequestModel(BaseModel):
    id: int | None = None
    lead_id: int
    step_id: int
    telegram_id: int
    proof_url: str
    lead_context: dict[str, Any] = {}
    status: str = "pending"
    result: dict[str, Any] | None = None
    created_at: str | None = None
    updated_at: str | None = None
```

**3. Add DraftRequestRepo to `bot/storage/repositories.py`:**

Add import of `DraftRequestModel` to the import block from `bot.storage.models`.

Add the class at the end of the file:

```python
class DraftRequestRepo:
    """Repository for draft generation request queue."""

    def __init__(self, client: InsForgeClient) -> None:
        self.client = client
        self.table = "draft_requests"

    async def claim_next_pending(self) -> DraftRequestModel | None:
        """Atomically claim the oldest pending request by setting status to 'processing'.

        Uses a two-step approach: query pending, then update with status filter
        to prevent race conditions (if another instance grabbed it, the update
        returns no rows).
        """
        rows = await self.client.query(
            self.table,
            filters={"status": "eq.pending"},
            order="created_at.asc",
            limit=1,
        )
        if not rows or not isinstance(rows, list) or len(rows) == 0:
            return None

        row = rows[0]
        # Attempt atomic claim: update only if still pending
        result = await self.client.update(
            self.table,
            filters={"id": row["id"], "status": "pending"},
            data={"status": "processing", "updated_at": datetime.now(timezone.utc).isoformat()},
        )
        if not result:
            return None  # Another instance claimed it
        return DraftRequestModel(**result)

    async def complete(self, request_id: int, result: dict[str, Any]) -> None:
        """Mark a request as completed with the generation result."""
        await self.client.update(
            self.table,
            filters={"id": request_id},
            data={
                "status": "completed",
                "result": result,
                "updated_at": datetime.now(timezone.utc).isoformat(),
            },
        )

    async def fail(self, request_id: int, error: str) -> None:
        """Mark a request as failed with error details."""
        await self.client.update(
            self.table,
            filters={"id": request_id},
            data={
                "status": "failed",
                "result": {"error": error},
                "updated_at": datetime.now(timezone.utc).isoformat(),
            },
        )

    async def reset_stale_processing(self, max_age_minutes: int = 2) -> int:
        """Reset processing requests older than max_age_minutes back to pending.

        Called on bot startup to recover from crashes during processing.
        Returns the number of requests reset.
        """
        cutoff = datetime.now(timezone.utc)
        # Query processing requests
        rows = await self.client.query(
            self.table,
            filters={"status": "eq.processing"},
        )
        if not rows or not isinstance(rows, list):
            return 0

        count = 0
        for row in rows:
            updated_at = datetime.fromisoformat(row["updated_at"].replace("Z", "+00:00"))
            age_minutes = (cutoff - updated_at).total_seconds() / 60
            if age_minutes > max_age_minutes:
                await self.client.update(
                    self.table,
                    filters={"id": row["id"]},
                    data={"status": "pending", "updated_at": cutoff.isoformat()},
                )
                count += 1
        return count
```

The `claim_next_pending` method uses a two-step approach (query then conditional update) which provides race condition safety — if two bot instances query the same row, only one's update will succeed since the other finds status != 'pending'.
  </action>
  <verify>
Check that `insforge/migrations/006_draft_requests.sql` exists with CREATE TABLE.
Check that `bot/storage/models.py` contains `class DraftRequestModel`.
Check that `bot/storage/repositories.py` contains `class DraftRequestRepo` with `claim_next_pending`, `complete`, `fail`, and `reset_stale_processing` methods.
Run `cd /Users/dmytrolevin/Downloads/GD_playground && python3 -c "from bot.storage.repositories import DraftRequestRepo; print('OK')"` to verify import.
  </verify>
  <done>
Migration SQL creates draft_requests table with status check constraint and two indexes. DraftRequestModel Pydantic model exists. DraftRequestRepo has claim_next_pending (race-safe), complete, fail, and reset_stale_processing methods. Python import succeeds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create CommentGeneratorAgent with multi-platform prompt</name>
  <files>
    bot/agents/comment_generator.py
    prompts/comment_generator_agent.md
  </files>
  <action>
**1. Create the prompt file** `prompts/comment_generator_agent.md`:

```markdown
# Comment Generator Agent — System Prompt

You are a contextual response generator for sales engagement. You receive a screenshot of a digital communication (social media post, email thread, DM conversation, etc.) along with lead context, and generate appropriate response drafts.

## Your Process

1. **Auto-detect the platform and content type** from the screenshot:
   - LinkedIn post/article → comment
   - LinkedIn DM → direct message reply
   - Email thread → email reply
   - Twitter/X post → reply tweet
   - Slack message → thread reply
   - Facebook post → comment
   - WhatsApp/Telegram message → message reply
   - Other → general response

2. **Analyze the content** visible in the screenshot:
   - What is the main topic?
   - What tone is the author using? (formal, casual, technical, personal)
   - What emotional register? (excited, concerned, analytical, celebratory)
   - Are there specific claims, questions, or discussion points?

3. **Generate three response options** of increasing length, appropriately formatted for the detected platform.

## Output Format

You MUST return valid JSON (no markdown formatting, no code fences, just raw JSON):

{
  "platform": "linkedin|email|twitter|slack|facebook|whatsapp|telegram|other",
  "content_type": "post_comment|dm_reply|email_reply|tweet_reply|thread_reply|message_reply|general_response",
  "options": [
    {
      "label": "Short",
      "length": "1-2 sentences",
      "text": "The short response text"
    },
    {
      "label": "Medium",
      "length": "2-3 sentences",
      "text": "The medium response text"
    },
    {
      "label": "Detailed",
      "length": "3-5 sentences",
      "text": "The detailed response text"
    }
  ]
}

## Response Rules

1. **Match the platform's communication style:**
   - LinkedIn: professional but warm, add value, no hard sells
   - Email: proper greeting/closing, match formality level of the thread
   - Twitter/X: concise, punchy, may use thread format for longer thoughts
   - Slack: casual but competent, can use emoji sparingly
   - DMs: conversational, direct, relationship-building

2. **Reference specifics** from the visible content — never generate generic responses

3. **Add unique value** — share a relevant insight, data point, experience, or thoughtful question

4. **Never use cliches:** "Great post!", "Couldn't agree more!", "Thanks for sharing!", "This is so important!"

5. **Never sell or pitch** — position the user as a knowledgeable peer, not a salesperson

6. **Match the original tone** — if the content is casual, respond casually; if analytical, respond analytically

7. **Use plain text only** — no markdown, no bullet points, no formatting (these don't render on most platforms)

8. **If lead context is provided**, subtly incorporate relevant knowledge (e.g., shared industry, common connections, relevant experience) without making it obvious you researched them

## Lead Context Usage

When lead context is provided (name, title, company, web research):
- Use their industry knowledge to make responses more relevant
- Reference shared professional interests when natural
- Adapt language complexity to match their professional level
- Do NOT mention their name, company, or personal details in the response (it's a public comment/reply, not a direct message — unless the platform IS a DM)
```

**2. Create the agent** `bot/agents/comment_generator.py`:

Follow the exact pattern from `extraction.py`:

```python
"""Comment Generator Agent — multi-platform draft generation from screenshots."""

from __future__ import annotations

import json
import logging
from pathlib import Path

from bot.agents.base import AgentInput, AgentOutput, BaseAgent
from bot.pipeline.context import PipelineContext
from langfuse import observe

logger = logging.getLogger(__name__)

_PROMPT_PATH = Path(__file__).resolve().parent.parent.parent / "prompts" / "comment_generator_agent.md"


class CommentGeneratorAgent(BaseAgent):
    """Generates contextual response drafts from screenshots.

    Auto-detects platform (LinkedIn, email, Twitter/X, Slack, DM, etc.)
    and generates appropriately formatted response options in three lengths.

    Uses vision model to analyze the screenshot content and lead context
    to produce relevant, authentic responses.
    """

    name = "comment_generator"

    def __init__(self) -> None:
        self._prompt: str = ""
        if _PROMPT_PATH.exists():
            self._prompt = _PROMPT_PATH.read_text(encoding="utf-8")
        else:
            logger.warning("Comment generator prompt not found: %s", _PROMPT_PATH)
            self._prompt = (
                "Analyze the screenshot and generate 3 contextual response options "
                "(short, medium, detailed). Return JSON with platform, content_type, "
                "and options array."
            )

    @observe(name="agent:comment_generator")
    async def run(self, input_data: AgentInput, pipeline_ctx: PipelineContext) -> AgentOutput:
        """Generate draft responses from a screenshot."""
        try:
            if not pipeline_ctx.image_b64:
                return AgentOutput(success=False, error="No image provided for draft generation")

            # Build user message with lead context
            lead_context = input_data.context.get("lead_context", {})
            parts = []
            parts.append("Analyze this screenshot and generate contextual response options.")
            parts.append("")

            if lead_context:
                parts.append("**Lead Context:**")
                if lead_context.get("name"):
                    parts.append(f"- Name: {lead_context['name']}")
                if lead_context.get("title"):
                    parts.append(f"- Title: {lead_context['title']}")
                if lead_context.get("company"):
                    parts.append(f"- Company: {lead_context['company']}")
                if lead_context.get("status"):
                    parts.append(f"- Relationship: {lead_context['status']}")
                if lead_context.get("web_research"):
                    parts.append(f"\n**Background Research:**\n{lead_context['web_research']}")

            user_message = "\n".join(parts)

            result = await pipeline_ctx.llm.complete(
                self._prompt,
                user_message,
                image_b64=pipeline_ctx.image_b64,
            )

            # Validate expected structure
            if isinstance(result, dict) and "options" in result:
                # Ensure options is a list with expected fields
                options = result.get("options", [])
                if isinstance(options, list) and len(options) > 0:
                    return AgentOutput(success=True, data=result)

            # If we got a dict but without proper options structure,
            # check if there's a raw_response we can report
            if isinstance(result, dict) and "raw_response" in result:
                logger.warning(
                    "CommentGenerator got raw_response instead of structured JSON: %s",
                    str(result.get("raw_response", ""))[:200],
                )
                return AgentOutput(
                    success=False,
                    error="AI did not return structured draft options. Try regenerating.",
                    data=result,
                )

            # Unexpected structure
            logger.warning("CommentGenerator unexpected result structure: %s", type(result))
            return AgentOutput(success=False, error="Unexpected response format from AI")

        except Exception as e:
            logger.error("CommentGeneratorAgent error: %s", e)
            return AgentOutput(success=False, error=str(e))
```

The agent follows the exact same pattern as ExtractionAgent:
- `name = "comment_generator"` for registry and ModelConfigService lookups
- Prompt loaded from `prompts/` directory at init
- `@observe(name="agent:comment_generator")` for Langfuse tracing
- Uses `pipeline_ctx.llm.complete()` with `image_b64` parameter
- Returns `AgentOutput` with structured data or error
  </action>
  <verify>
Run `cd /Users/dmytrolevin/Downloads/GD_playground && python3 -c "from bot.agents.comment_generator import CommentGeneratorAgent; a = CommentGeneratorAgent(); print(f'Agent: {a.name}, prompt loaded: {len(a._prompt) > 0}')"` — should print agent name and confirm prompt loaded.
Verify `prompts/comment_generator_agent.md` exists and contains "auto-detect".
Verify the agent has `@observe` decorator and uses `pipeline_ctx.llm.complete()` with `image_b64`.
  </verify>
  <done>
CommentGeneratorAgent exists with `name = "comment_generator"`, loads prompt from `prompts/comment_generator_agent.md`, has `@observe` decorator for Langfuse, uses vision model via `pipeline_ctx.llm.complete()` with `image_b64`, validates structured JSON output with options array, and handles error cases. Prompt covers multi-platform auto-detection for LinkedIn, email, Twitter/X, Slack, DMs, and generic content.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create draft request poller and wire into main.py</name>
  <files>
    bot/services/draft_poller.py
    bot/main.py
  </files>
  <action>
**1. Create the poller service** `bot/services/draft_poller.py`:

```python
"""Background poller for draft generation requests (TMA -> Bot async message bus).

Polls the draft_requests table every 3 seconds for pending requests,
processes them through CommentGeneratorAgent, and writes results back.
"""

from __future__ import annotations

import asyncio
import base64
import logging

import httpx

from bot.agents.base import AgentInput
from bot.agents.registry import AgentRegistry
from bot.pipeline.context import PipelineContext
from bot.services.image_utils import pre_resize_image
from bot.services.llm_router import create_provider
from bot.services.model_config import ModelConfigService
from bot.storage.repositories import DraftRequestRepo

logger = logging.getLogger(__name__)

POLL_INTERVAL = 3  # seconds


async def _fetch_and_encode_image(proof_url: str) -> str | None:
    """Fetch image from URL, pre-resize for vision model, and base64-encode."""
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            resp = await client.get(proof_url)
            resp.raise_for_status()
            image_bytes = resp.content

        # Pre-resize for vision model (max 1568px, JPEG quality 85)
        resized = pre_resize_image(image_bytes)

        return base64.b64encode(resized).decode("ascii")
    except Exception as e:
        logger.error("Failed to fetch/encode image from %s: %s", proof_url, e)
        return None


async def _process_draft_request(
    request,
    agent_registry: AgentRegistry,
    model_config_service: ModelConfigService,
    draft_repo: DraftRequestRepo,
    shared_openrouter_key: str,
) -> None:
    """Process a single draft request through the CommentGeneratorAgent."""
    try:
        # 1. Fetch and encode the proof screenshot
        image_b64 = await _fetch_and_encode_image(request.proof_url)
        if not image_b64:
            await draft_repo.fail(request.id, "Failed to fetch screenshot image")
            return

        # 2. Get the agent
        agent = agent_registry.get("comment_generator")

        # 3. Build a lightweight PipelineContext for the agent
        #    Use shared OpenRouter key as default provider
        default_llm = create_provider("openrouter", shared_openrouter_key)

        ctx = PipelineContext(
            llm=default_llm,
            image_b64=image_b64,
            telegram_id=request.telegram_id,
            model_config=model_config_service,
        )

        # 4. Resolve per-agent model override (ModelConfigService)
        override_llm = await ctx.get_llm_for_agent(agent.name)
        if override_llm:
            ctx.llm = override_llm

        # 5. Build agent input with lead context
        agent_input = AgentInput(
            user_message="Generate contextual response options from this screenshot.",
            context={"lead_context": request.lead_context or {}},
        )

        # 6. Run the agent
        output = await agent.run(agent_input, ctx)

        # 7. Write result back
        if output.success:
            await draft_repo.complete(request.id, output.data)
            logger.info(
                "Draft request %d completed for lead %d step %d",
                request.id, request.lead_id, request.step_id,
            )
        else:
            await draft_repo.fail(request.id, output.error or "Agent returned unsuccessful result")
            logger.warning(
                "Draft request %d failed for lead %d: %s",
                request.id, request.lead_id, output.error,
            )

        # 8. Close the temporary provider to prevent connection leaks
        try:
            await default_llm.close()
        except Exception:
            pass

    except Exception as e:
        logger.error("Draft request %d processing error: %s", request.id, e)
        try:
            await draft_repo.fail(request.id, str(e))
        except Exception:
            logger.error("Failed to mark draft request %d as failed", request.id)


async def start_draft_request_poller(
    agent_registry: AgentRegistry,
    model_config_service: ModelConfigService,
    draft_repo: DraftRequestRepo,
    shared_openrouter_key: str,
) -> None:
    """Poll draft_requests table and process pending requests.

    Runs as an infinite loop background task. On startup, recovers
    any stale 'processing' requests back to 'pending'.
    """
    # Recover stale requests from previous crashes
    try:
        recovered = await draft_repo.reset_stale_processing(max_age_minutes=2)
        if recovered:
            logger.info("Recovered %d stale draft requests back to pending", recovered)
    except Exception as e:
        logger.error("Failed to recover stale draft requests: %s", e)

    logger.info("Draft request poller started (interval: %ds)", POLL_INTERVAL)

    while True:
        try:
            request = await draft_repo.claim_next_pending()
            if request:
                await _process_draft_request(
                    request,
                    agent_registry,
                    model_config_service,
                    draft_repo,
                    shared_openrouter_key,
                )
        except Exception as e:
            logger.error("Draft poller iteration error: %s", e)

        await asyncio.sleep(POLL_INTERVAL)
```

**2. Wire into `bot/main.py`:**

Add these imports at the top (in the appropriate import sections):

```python
from bot.agents.comment_generator import CommentGeneratorAgent
from bot.services.draft_poller import start_draft_request_poller
```

Add `DraftRequestRepo` to the existing import from `bot.storage.repositories`.

In the agent registry section (after the existing `agent_registry.register(ReanalysisStrategistAgent())`), add:

```python
agent_registry.register(CommentGeneratorAgent())
```

In the repository initialization section (after `model_config_repo = AgentModelConfigRepo(insforge)`), add:

```python
draft_request_repo = DraftRequestRepo(insforge)
```

In the background tasks section (after the plan_scheduler block, still inside `if engagement_service:`), add:

```python
        # Start draft request poller for TMA draft generation
        create_background_task(
            start_draft_request_poller(
                agent_registry,
                model_config_service,
                draft_request_repo,
                cfg.openrouter_api_key,
            ),
            name="draft_request_poller",
        )
        logger.info("Draft request poller started (3-second interval)")
```

Note: The poller lives inside the `if engagement_service:` block because it needs the shared OpenRouter key. If no key is set, draft generation won't work (same as other engagement features).
  </action>
  <verify>
Run `cd /Users/dmytrolevin/Downloads/GD_playground && python3 -c "from bot.services.draft_poller import start_draft_request_poller; print('OK')"` — import succeeds.
Verify `bot/main.py` imports `CommentGeneratorAgent` and `start_draft_request_poller`.
Verify `bot/main.py` registers `CommentGeneratorAgent()` in agent_registry.
Verify `bot/main.py` creates `DraftRequestRepo(insforge)`.
Verify `bot/main.py` starts `draft_request_poller` as a background task.
Grep for "comment_generator" in main.py to confirm agent registration.
Grep for "draft_request_poller" in main.py to confirm background task.
  </verify>
  <done>
Draft poller service exists with 3-second polling interval, image fetch+resize+encode pipeline, PipelineContext creation with ModelConfigService, per-agent model override resolution, and structured result writing. Stale request recovery runs on startup. main.py registers CommentGeneratorAgent, creates DraftRequestRepo, and starts draft_request_poller as a background task. All imports resolve correctly.
  </done>
</task>

</tasks>

<verification>
- `python3 -c "from bot.agents.comment_generator import CommentGeneratorAgent; from bot.services.draft_poller import start_draft_request_poller; from bot.storage.repositories import DraftRequestRepo; print('All imports OK')"` succeeds
- `insforge/migrations/006_draft_requests.sql` exists with CREATE TABLE and indexes
- `prompts/comment_generator_agent.md` exists with multi-platform detection instructions
- `bot/main.py` registers the agent, creates the repo, and starts the poller
- The agent has `@observe` decorator and uses `pipeline_ctx.llm.complete()` with `image_b64`
- The poller recovers stale requests on startup and processes pending requests through the agent
</verification>

<success_criteria>
- CommentGeneratorAgent is a registered bot agent with vision model support
- Admin can configure its model via ModelConfigService (same as all other agents)
- Langfuse traces all draft generations automatically via @observe
- Multi-platform prompt auto-detects LinkedIn, email, Twitter/X, Slack, DMs, and other platforms
- Agent returns structured JSON with platform, content_type, and options array (3 lengths)
- Draft poller background task runs every 3 seconds, claims pending requests, processes them, writes results
- Stale processing requests are recovered to pending on bot startup
- No new dependencies added (all existing: httpx, langfuse, Pillow)
</success_criteria>

<output>
After completion, create `.planning/phases/19-active-engagement-execution/19-04-SUMMARY.md`
</output>
