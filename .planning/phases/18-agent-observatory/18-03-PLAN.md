---
phase: 18-agent-observatory
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - insforge/migrations/005_agent_model_config.sql
  - bot/storage/models.py
  - bot/storage/repositories.py
  - bot/services/model_config.py
  - bot/pipeline/context.py
  - bot/pipeline/runner.py
  - bot/main.py
autonomous: true

must_haves:
  truths:
    - "Admin can set per-agent model overrides that are stored in the database and take effect within 60 seconds (no restart needed)"
    - "Agents with overrides use the specified OpenRouter model via the shared API key; agents without overrides use the user's default provider"
    - "PipelineRunner resolves agent-specific models before each agent run, creating correct LLM provider per agent"
  artifacts:
    - path: "insforge/migrations/005_agent_model_config.sql"
      provides: "agent_model_config table definition"
      contains: "CREATE TABLE agent_model_config"
    - path: "bot/services/model_config.py"
      provides: "ModelConfigService with 60s TTL cache"
      contains: "class ModelConfigService"
    - path: "bot/pipeline/context.py"
      provides: "PipelineContext with get_llm_for_agent method"
      contains: "get_llm_for_agent"
    - path: "bot/pipeline/runner.py"
      provides: "PipelineRunner that resolves per-agent models"
      contains: "get_llm_for_agent"
  key_links:
    - from: "bot/pipeline/runner.py"
      to: "bot/pipeline/context.py"
      via: "ctx.get_llm_for_agent(step.agent)"
      pattern: "get_llm_for_agent"
    - from: "bot/pipeline/context.py"
      to: "bot/services/model_config.py"
      via: "ModelConfigService.get_override(agent_name)"
      pattern: "get_override"
    - from: "bot/services/model_config.py"
      to: "bot/storage/repositories.py"
      via: "AgentModelConfigRepo.get_all_active()"
      pattern: "agent_model_config"
---

<objective>
Create the per-agent model configuration system: database table, repository, cached service, and PipelineContext/Runner refactoring so each agent can use a different LLM model.

Purpose: Enables admin to configure which OpenRouter model each agent uses (e.g., strategist on claude-sonnet, extraction on gpt-4o-mini) without code deploys. Changes take effect within 60 seconds via TTL cache.
Output: Migration SQL, AgentModelConfigRepo, ModelConfigService, refactored PipelineContext with get_llm_for_agent(), updated PipelineRunner.
</objective>

<execution_context>
@/Users/dmytrolevin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dmytrolevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-agent-observatory/18-RESEARCH.md
@bot/storage/models.py
@bot/storage/repositories.py
@bot/storage/insforge_client.py
@bot/services/llm_router.py
@bot/pipeline/context.py
@bot/pipeline/runner.py
@bot/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Database migration, model, repository, and ModelConfigService</name>
  <files>insforge/migrations/005_agent_model_config.sql, bot/storage/models.py, bot/storage/repositories.py, bot/services/model_config.py</files>
  <action>
1. **Create insforge/migrations/005_agent_model_config.sql:**
```sql
-- Per-agent model configuration for admin overrides
CREATE TABLE IF NOT EXISTS agent_model_config (
    id SERIAL PRIMARY KEY,
    agent_name TEXT NOT NULL UNIQUE,
    model_id TEXT NOT NULL,
    is_active BOOLEAN NOT NULL DEFAULT true,
    set_by TEXT,
    created_at TIMESTAMPTZ DEFAULT now(),
    updated_at TIMESTAMPTZ DEFAULT now()
);

-- Fast lookup by agent name
CREATE INDEX IF NOT EXISTS idx_agent_model_config_agent ON agent_model_config(agent_name) WHERE is_active = true;
```
NOTE: Research suggested provider field but the design decision is to use shared OpenRouter key ONLY for admin overrides (avoids cross-provider issues). So provider is always "openrouter" -- no need for a provider column.

2. **Add AgentModelConfigModel to bot/storage/models.py:**
```python
class AgentModelConfigModel(BaseModel):
    """Per-agent model configuration set by admin."""
    id: int | None = None
    agent_name: str
    model_id: str
    is_active: bool = True
    set_by: str | None = None
    created_at: str | None = None
    updated_at: str | None = None
```

3. **Add AgentModelConfigRepo to bot/storage/repositories.py:**
```python
class AgentModelConfigRepo:
    """Repository for agent_model_config table."""

    def __init__(self, client: InsForgeClient) -> None:
        self._client = client

    async def get_all_active(self) -> list[dict]:
        """Get all active agent model configs."""
        return await self._client.query(
            "agent_model_config",
            filters={"is_active": "eq.true"},
        ) or []

    async def upsert(self, agent_name: str, model_id: str, set_by: str | None = None) -> dict | None:
        """Create or update a model config for an agent."""
        # Check if exists
        existing = await self._client.query(
            "agent_model_config",
            filters={"agent_name": f"eq.{agent_name}"},
        )
        if existing:
            return await self._client.update(
                "agent_model_config",
                filters={"agent_name": f"eq.{agent_name}"},
                data={
                    "model_id": model_id,
                    "is_active": True,
                    "set_by": set_by,
                    "updated_at": datetime.now(timezone.utc).isoformat(),
                },
            )
        else:
            return await self._client.insert(
                "agent_model_config",
                data={
                    "agent_name": agent_name,
                    "model_id": model_id,
                    "is_active": True,
                    "set_by": set_by,
                },
            )

    async def deactivate(self, agent_name: str) -> dict | None:
        """Deactivate (soft delete) a model config."""
        return await self._client.update(
            "agent_model_config",
            filters={"agent_name": f"eq.{agent_name}"},
            data={"is_active": False, "updated_at": datetime.now(timezone.utc).isoformat()},
        )
```
Add the import of `AgentModelConfigModel` to the models import block at the top of repositories.py. Also add `AgentModelConfigRepo` to the module.

4. **Create bot/services/model_config.py:**
```python
"""Per-agent model configuration service with in-memory caching."""

import logging
import time
from typing import Any

from bot.storage.repositories import AgentModelConfigRepo
from bot.services.llm_router import LLMProvider, create_provider

logger = logging.getLogger(__name__)


class ModelConfigService:
    """Manages per-agent model overrides with 60-second TTL cache."""

    CACHE_TTL = 60  # seconds

    def __init__(self, config_repo: AgentModelConfigRepo, shared_openrouter_key: str) -> None:
        self._repo = config_repo
        self._shared_key = shared_openrouter_key
        self._cache: dict[str, dict[str, Any]] = {}
        self._cache_time: float = 0
        # Cache provider instances by model_id to avoid creating new httpx clients per call
        self._provider_cache: dict[str, LLMProvider] = {}

    async def get_override(self, agent_name: str) -> dict | None:
        """Get model override for a specific agent, or None if no override."""
        await self._refresh_if_stale()
        return self._cache.get(agent_name)

    async def get_all(self) -> dict[str, dict[str, Any]]:
        """Get all active model overrides."""
        await self._refresh_if_stale()
        return dict(self._cache)

    async def get_provider_for_agent(self, agent_name: str) -> LLMProvider | None:
        """Get an LLM provider for an agent's override, or None for default.

        Provider instances are cached by model_id to reuse httpx connections.
        """
        override = await self.get_override(agent_name)
        if not override or not self._shared_key:
            return None

        model_id = override["model_id"]
        if model_id not in self._provider_cache:
            self._provider_cache[model_id] = create_provider(
                "openrouter", self._shared_key, model=model_id
            )
        return self._provider_cache[model_id]

    async def _refresh_if_stale(self) -> None:
        """Refresh cache from database if TTL expired."""
        if time.time() - self._cache_time < self.CACHE_TTL:
            return
        try:
            rows = await self._repo.get_all_active()
            self._cache = {r["agent_name"]: r for r in rows}
            self._cache_time = time.time()
            logger.debug("Model config cache refreshed: %d overrides", len(self._cache))
        except Exception as e:
            logger.error("Failed to refresh model config cache: %s", e)
            # Keep stale cache rather than failing

    async def close(self) -> None:
        """Close cached provider instances."""
        for provider in self._provider_cache.values():
            try:
                await provider.close()
            except Exception:
                pass
        self._provider_cache.clear()
```

DESIGN DECISION: Admin model overrides ALWAYS use the shared OpenRouter API key (from cfg.openrouter_api_key). This avoids the cross-provider problem (user has Claude key but admin wants to use GPT-4o). Admin overrides are admin-funded via the shared key.
  </action>
  <verify>
    - `cat insforge/migrations/005_agent_model_config.sql` shows the CREATE TABLE
    - `python -c "from bot.storage.models import AgentModelConfigModel; print('OK')"` succeeds
    - `python -c "from bot.storage.repositories import AgentModelConfigRepo; print('OK')"` succeeds
    - `python -c "from bot.services.model_config import ModelConfigService; print('OK')"` succeeds
  </verify>
  <done>Migration SQL ready, Pydantic model defined, repository with upsert/deactivate, ModelConfigService with 60s TTL cache and provider instance caching.</done>
</task>

<task type="auto">
  <name>Task 2: Refactor PipelineContext and PipelineRunner for per-agent model resolution</name>
  <files>bot/pipeline/context.py, bot/pipeline/runner.py, bot/main.py</files>
  <action>
1. **Refactor bot/pipeline/context.py:**
   Add a `model_config` parameter and `get_llm_for_agent()` method:

   - Add `model_config: ModelConfigService | None = None` parameter to `__init__`
   - Store as `self._model_config = model_config`
   - Rename `self.llm` to `self.default_llm` internally BUT keep `self.llm` as an alias property for backward compatibility (agents currently access `pipeline_ctx.llm.complete()`)
   - Add method:
     ```python
     async def get_llm_for_agent(self, agent_name: str) -> LLMProvider:
         """Get LLM provider for a specific agent (override or default)."""
         if self._model_config:
             override_provider = await self._model_config.get_provider_for_agent(agent_name)
             if override_provider:
                 return override_provider
         return self.default_llm
     ```
   - The `llm` property should return `self.default_llm` for backward compatibility with existing agent code that does `pipeline_ctx.llm.complete()`

   IMPORTANT: Existing agents (strategist, trainer, etc.) access `pipeline_ctx.llm` directly. Keep this working. The `get_llm_for_agent()` method is called by the PipelineRunner to resolve the correct provider, and the runner passes it to the agent via a temporary override on `self.llm`.

2. **Refactor bot/pipeline/runner.py:**
   Modify `_run_step()` to resolve per-agent models:

   ```python
   async def _run_step(self, step: StepConfig, ctx: PipelineContext) -> AgentOutput:
       agent = self.registry.get(step.agent)
       agent_input = self._build_input(step, ctx)

       logger.info("Running agent: %s (sequential)", step.agent)

       # Resolve per-agent model override
       original_llm = ctx.default_llm
       try:
           override_llm = await ctx.get_llm_for_agent(step.agent)
           ctx.default_llm = override_llm  # Temporarily swap so agent sees override via ctx.llm

           output = await agent.run(agent_input, ctx)
           ctx.set_result(step.agent, output)
           logger.info("Agent %s completed: success=%s", step.agent, output.success)
           return output
       except Exception as e:
           logger.error("Agent %s failed: %s", step.agent, e)
           error_output = AgentOutput(success=False, error=str(e))
           ctx.set_result(step.agent, error_output)
           return error_output
       finally:
           ctx.default_llm = original_llm  # Restore default
   ```

   Apply the same pattern to `_run_parallel()`:
   ```python
   async def _run_one(step: StepConfig) -> tuple[str, AgentOutput]:
       agent = self.registry.get(step.agent)
       agent_input = self._build_input(step, ctx)
       try:
           override_llm = await ctx.get_llm_for_agent(step.agent)
           # For parallel, create a temporary context attribute
           # Since parallel agents may run concurrently, we need to pass the LLM differently
           # Store override in ctx.results temporarily
           original = ctx.default_llm
           ctx.default_llm = override_llm
           output = await agent.run(agent_input, ctx)
           ctx.default_llm = original
           return step.agent, output
       except Exception as e:
           logger.error("Parallel agent %s failed: %s", step.agent, e)
           return step.agent, AgentOutput(success=False, error=str(e))
   ```

   NOTE: For parallel execution, the temporary swap is not safe since multiple agents run concurrently. A better approach for _run_parallel: instead of swapping ctx.default_llm, pass the override provider via a wrapper. However, since the current agents access `pipeline_ctx.llm` directly, the cleanest approach is:

   For _run_parallel, resolve all override LLMs before launching concurrent tasks, then create a shallow copy of the context for each parallel agent:
   ```python
   async def _run_parallel(self, steps: list[StepConfig], ctx: PipelineContext) -> None:
       async def _run_one(step: StepConfig) -> tuple[str, AgentOutput]:
           agent = self.registry.get(step.agent)
           agent_input = self._build_input(step, ctx)
           try:
               override_llm = await ctx.get_llm_for_agent(step.agent)
               # Save/restore for this agent's execution
               saved = ctx.default_llm
               ctx.default_llm = override_llm
               try:
                   output = await agent.run(agent_input, ctx)
               finally:
                   ctx.default_llm = saved
               return step.agent, output
           except Exception as e:
               return step.agent, AgentOutput(success=False, error=str(e))

       results = await asyncio.gather(*[_run_one(s) for s in steps])
       for agent_name, output in results:
           ctx.set_result(agent_name, output)
   ```
   NOTE: In practice, asyncio.gather with await-based async functions in Python means only one coroutine runs at a time (they interleave on I/O). The save/restore approach works because the swap happens before the first I/O yield in agent.run(). The agents' LLM call is the first significant I/O, and by then the correct LLM is set. This is safe because Python async is cooperative, not preemptive.

   Also update `_run_background()` to resolve the model before spawning:
   ```python
   def _run_background(self, step: StepConfig, ctx: PipelineContext) -> None:
       async def _bg_task() -> None:
           try:
               agent = self.registry.get(step.agent)
               agent_input = self._build_input(step, ctx)
               override_llm = await ctx.get_llm_for_agent(step.agent)
               saved = ctx.default_llm
               ctx.default_llm = override_llm
               try:
                   output = await agent.run(agent_input, ctx)
               finally:
                   ctx.default_llm = saved
               ctx.set_result(step.agent, output)
           except Exception as e:
               logger.error("Background agent %s failed: %s", step.agent, e)
       create_background_task(_bg_task(), name=f"bg_agent_{step.agent}")
   ```

3. **Wire into bot/main.py:**
   - Add import: `from bot.services.model_config import ModelConfigService`
   - Add import: `from bot.storage.repositories import AgentModelConfigRepo` (add to existing import block)
   - After creating `reminder_repo`, add:
     ```python
     model_config_repo = AgentModelConfigRepo(insforge)
     ```
   - After creating engagement_service, add:
     ```python
     model_config_service = ModelConfigService(model_config_repo, cfg.openrouter_api_key)
     if cfg.openrouter_api_key:
         logger.info("Model config service initialized (per-agent overrides enabled)")
     else:
         logger.info("Model config service initialized (no shared key, overrides will be no-op)")
     ```
   - Add `model_config_service` to `dp.workflow_data`:
     ```python
     "model_config_service": model_config_service,
     ```
   - In the `finally` block, add `await model_config_service.close()` before `await insforge.close()`

4. **Update handler PipelineContext construction:**
   In all handlers that create PipelineContext (support.py, learn.py, train.py), the model_config_service needs to be passed. Since it's in workflow_data, handlers access it via parameter injection.

   **Scope boundary:** This plan (18-03) ONLY touches the PipelineContext class signature (adds `model_config=None` parameter) and PipelineRunner internals. It does NOT modify any handler files. All handler call-site updates (passing model_config_service to PipelineContext constructors) are Plan 02's responsibility, since Plan 02 already rewrites handler pipeline sections for Langfuse wiring.

   For THIS plan: ensure PipelineContext.__init__ has `model_config=None` as optional param, and get_llm_for_agent handles None gracefully.
  </action>
  <verify>
    - `python -c "from bot.pipeline.context import PipelineContext; print('OK')"` succeeds
    - `python -c "from bot.pipeline.runner import PipelineRunner; print('OK')"` succeeds
    - `python -c "from bot.services.model_config import ModelConfigService; print('OK')"` succeeds
    - `grep "get_llm_for_agent" bot/pipeline/context.py` shows the method
    - `grep "get_llm_for_agent" bot/pipeline/runner.py` shows it's called in _run_step
    - `grep "model_config_service" bot/main.py` shows DI wiring
    - `cat insforge/migrations/005_agent_model_config.sql` shows valid SQL
  </verify>
  <done>PipelineContext has get_llm_for_agent() method, PipelineRunner resolves per-agent models before each step, ModelConfigService is wired into main.py DI, migration SQL is ready to run. Agent code is untouched (backward compatible via ctx.llm property).</done>
</task>

</tasks>

<verification>
- `python -c "from bot.pipeline.context import PipelineContext; from bot.pipeline.runner import PipelineRunner; from bot.services.model_config import ModelConfigService; print('All imports OK')"` succeeds
- PipelineContext backward compatible: `ctx.llm` still works
- PipelineContext new API: `await ctx.get_llm_for_agent('strategist')` works
- ModelConfigService handles missing shared key gracefully (returns None, uses default)
- Migration SQL is valid and creates agent_model_config table
</verification>

<success_criteria>
- agent_model_config table migration ready
- AgentModelConfigRepo with get_all_active, upsert, deactivate
- ModelConfigService with 60s TTL cache and provider instance caching
- PipelineContext.get_llm_for_agent() returns override provider or default
- PipelineRunner resolves per-agent models in sequential, parallel, and background modes
- main.py creates and injects ModelConfigService
- All existing agent code continues to work via backward-compatible ctx.llm property
</success_criteria>

<output>
After completion, create `.planning/phases/18-agent-observatory/18-03-SUMMARY.md`
</output>
