---
phase: 18-agent-observatory
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - bot/config.py
  - bot/tracing/langfuse_setup.py
  - bot/services/llm_router.py
autonomous: true

must_haves:
  truths:
    - "Langfuse SDK is installed and initializes on bot startup without errors"
    - "Every LLM completion (Claude and OpenRouter) records a generation observation in Langfuse with model name, input, output, token counts, and cost"
    - "LANGFUSE_BASE_URL can be swapped to self-hosted URL without code changes"
  artifacts:
    - path: "requirements.txt"
      provides: "langfuse dependency"
      contains: "langfuse>=3.12.1"
    - path: "bot/config.py"
      provides: "Langfuse env var configuration"
      contains: "langfuse_public_key"
    - path: "bot/tracing/langfuse_setup.py"
      provides: "Langfuse client initialization"
      contains: "init_langfuse"
    - path: "bot/services/llm_router.py"
      provides: "LLM providers with Langfuse generation observations"
      contains: "observe"
  key_links:
    - from: "bot/tracing/langfuse_setup.py"
      to: "langfuse SDK"
      via: "env var auto-configuration"
      pattern: "LANGFUSE_PUBLIC_KEY"
    - from: "bot/services/llm_router.py"
      to: "langfuse"
      via: "@observe decorator on complete() methods"
      pattern: "@observe"
---

<objective>
Install Langfuse SDK, create initialization module, and instrument both LLM providers (ClaudeProvider and OpenRouterProvider) with Langfuse generation observations that capture model, input/output, token usage, and cost.

Purpose: This is the foundation for all AI observability. Without generation observations on the LLM providers, no trace data flows into Langfuse.
Output: langfuse installed, setup module created, both LLM providers emit generation observations with full metadata.
</objective>

<execution_context>
@/Users/dmytrolevin/.claude/get-shit-done/workflows/execute-plan.md
@/Users/dmytrolevin/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/18-agent-observatory/18-RESEARCH.md
@bot/services/llm_router.py
@bot/config.py
@bot/tracing/langfuse_setup.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Langfuse SDK and create initialization module</name>
  <files>requirements.txt, bot/config.py, bot/tracing/langfuse_setup.py</files>
  <action>
1. Add `langfuse>=3.12.1` to requirements.txt (after the existing dependencies).
2. Run `pip install langfuse>=3.12.1` to install in the current virtualenv.
3. Add Langfuse env vars to bot/config.py Settings class:
   - `langfuse_public_key: str = ""` (optional, observability disabled when empty)
   - `langfuse_secret_key: str = ""` (optional)
   - `langfuse_base_url: str = "https://cloud.langfuse.com"` (default to cloud, swap for self-hosted)
4. Create `bot/tracing/langfuse_setup.py`:
   - `init_langfuse(settings)` function that:
     - Checks if `settings.langfuse_public_key` is set; if empty, logs "Langfuse disabled (no LANGFUSE_PUBLIC_KEY)" and returns False
     - Sets os.environ["LANGFUSE_PUBLIC_KEY"], os.environ["LANGFUSE_SECRET_KEY"], and os.environ["LANGFUSE_BASE_URL"] from settings
       (the Langfuse SDK reads these env vars automatically via get_client())
     - Logs "Langfuse initialized (base_url={base_url})"
     - Returns True
   - `shutdown_langfuse()` function that calls `langfuse.flush()` via `get_client().flush()` if Langfuse is initialized, wrapped in try/except

IMPORTANT: The Langfuse SDK v3 uses `from langfuse import observe, get_client` as module-level imports. The SDK auto-configures from LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY, LANGFUSE_BASE_URL environment variables. Do NOT create a Langfuse() instance manually -- use the module-level `get_client()` / `@observe()` API.

IMPORTANT: Make langfuse_setup.py handle the "not configured" case gracefully. When keys are empty, all @observe decorators should be no-ops (Langfuse handles this -- when no keys are set, observations are simply not sent).
  </action>
  <verify>
    - `python -c "import langfuse; print(langfuse.__version__)"` succeeds and prints >= 3.12.1
    - `python -c "from bot.tracing.langfuse_setup import init_langfuse, shutdown_langfuse; print('OK')"` succeeds
    - `grep langfuse requirements.txt` shows the dependency
    - `grep langfuse_public_key bot/config.py` shows the env var
  </verify>
  <done>Langfuse SDK installed, Settings has 3 Langfuse env vars, init/shutdown functions exist and handle unconfigured case gracefully</done>
</task>

<task type="auto">
  <name>Task 2: Instrument LLM providers with Langfuse generation observations</name>
  <files>bot/services/llm_router.py</files>
  <action>
Modify both ClaudeProvider.complete() and OpenRouterProvider.complete() to emit Langfuse generation observations:

1. Replace `from bot.tracing import traced_span` with `from langfuse import observe, get_client` at the top of llm_router.py.

2. **ClaudeProvider.complete():**
   - Replace `@traced_span("llm:claude")` with `@observe(as_type="generation", name="llm:claude")`
   - After getting the successful response (`data = resp.json()`), extract usage:
     ```python
     usage = data.get("usage", {})
     input_tokens = usage.get("input_tokens", 0)
     output_tokens = usage.get("output_tokens", 0)
     ```
   - Call `get_client().update_current_observation(...)` with:
     - `model=self.model`
     - `input={"system": system_prompt[:500], "user": user_message[:500], "has_image": bool(image_b64)}` (truncate to avoid Langfuse data limits with 70K prompts)
     - `output=text[:2000]` (the raw LLM text output before JSON extraction)
     - `usage_details={"input": input_tokens, "output": output_tokens}` (Langfuse v3 format)
     - `metadata={"provider": "claude"}`
   - This must happen INSIDE the retry loop, after a successful response, BEFORE the return statement
   - Wrap the update_current_observation call in try/except to never break the LLM flow if Langfuse fails

3. **OpenRouterProvider.complete():**
   - Replace `@traced_span("llm:openrouter")` with `@observe(as_type="generation", name="llm:openrouter")`
   - After getting the successful response, extract usage and cost:
     ```python
     usage = data.get("usage", {})
     prompt_tokens = usage.get("prompt_tokens", 0)
     completion_tokens = usage.get("completion_tokens", 0)
     cost_value = usage.get("cost")  # OpenRouter provides cost directly
     ```
   - Call `get_client().update_current_observation(...)` with:
     - `model=self.model`
     - `input={"system": system_prompt[:500], "user": user_message[:500], "has_image": bool(image_b64)}`
     - `output=text[:2000]`
     - `usage_details={"input": prompt_tokens, "output": completion_tokens}`
     - `cost_details={"total": cost_value}` if cost_value else omit
     - `metadata={"provider": "openrouter"}`
   - Same safety: wrap in try/except, never break LLM flow

4. Keep the existing `_extract_json`, `create_provider`, `web_research_call` functions unchanged.

IMPORTANT: Use `get_client().update_current_observation()` (NOT `update_current_generation` -- that was v2 API). In Langfuse v3 with `@observe(as_type="generation")`, use `update_current_observation()` to set generation-specific fields.

IMPORTANT: Input truncation at 500 chars is critical. The strategist prompt includes ~70K of playbook text. Logging that would blow up Langfuse storage. Log a truncated preview + metadata flag. The admin can always check the full prompt in code.
  </action>
  <verify>
    - `python -c "from bot.services.llm_router import ClaudeProvider, OpenRouterProvider; print('OK')"` imports successfully
    - `grep -c "@observe" bot/services/llm_router.py` returns 2 (one per provider)
    - `grep "update_current_observation" bot/services/llm_router.py` shows observation updates in both providers
    - `grep "traced_span" bot/services/llm_router.py` returns nothing (fully replaced)
  </verify>
  <done>Both LLM providers use @observe(as_type="generation") decorator and emit model, truncated input/output, token usage, and cost (OpenRouter) to Langfuse. Old @traced_span removed. Observation updates are wrapped in try/except for safety.</done>
</task>

</tasks>

<verification>
- `pip show langfuse` shows version >= 3.12.1
- `python -c "from bot.services.llm_router import create_provider; from bot.tracing.langfuse_setup import init_langfuse; print('All imports OK')"` succeeds
- No references to `traced_span` remain in `bot/services/llm_router.py`
- `bot/config.py` has `langfuse_public_key`, `langfuse_secret_key`, `langfuse_base_url` fields
</verification>

<success_criteria>
- Langfuse SDK installed and importable
- init_langfuse/shutdown_langfuse handle both configured and unconfigured states
- ClaudeProvider.complete() and OpenRouterProvider.complete() emit Langfuse generation observations with model, truncated I/O, token usage, and cost
- Self-hosted migration requires only changing LANGFUSE_BASE_URL env var
</success_criteria>

<output>
After completion, create `.planning/phases/18-agent-observatory/18-01-SUMMARY.md`
</output>
